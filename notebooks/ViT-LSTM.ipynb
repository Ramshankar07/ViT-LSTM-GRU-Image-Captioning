{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL77oeyIz25f",
        "outputId": "6f91cbed-ef77-4a15-bf54-c207691313c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgGjZcqg0oYm",
        "outputId": "22b41c0a-7808-4ded-c058-920951080e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext, sentence-transformers\n",
            "Successfully installed sentence-transformers-3.2.1 torchtext-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext sentence-transformers transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUKomh96YrkY",
        "outputId": "a1a5a545-77d2-4e0c-c2c9-d37f989cd959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow)\n",
            "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.6.0 tensorboard-2.18.0 tensorflow-2.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahm0YtTHXcVt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import os\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qj1BXPFQy6Xp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdS9UUN5Xx6M",
        "outputId": "4dc7d73d-6849-4c07-ca3f-02a829bf35b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "EMBED_SIZE = 768  # ViT has 12 layers\n",
        "HIDDEN_SIZE = 512\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "    # Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_58yGRQyb0_F"
      },
      "outputs": [],
      "source": [
        "def extract_vit_features(image, feature_extractor, vit_model):\n",
        "    \"\"\"Extract features from ViT\"\"\"\n",
        "    with torch.no_grad():\n",
        "        inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "        outputs = vit_model(**inputs)\n",
        "        features = outputs.last_hidden_state[:, 0, :]\n",
        "        print(f\"ViT feature shape: {features.shape}\")\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CoOsC2gNy5vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46b08OSQXfx1"
      },
      "outputs": [],
      "source": [
        "class DataPreprocessor:\n",
        "    def __init__(self, image_dir, captions_file, feature_extractor, max_len=50, cache_dir='cached_data'):\n",
        "        self.image_dir = image_dir\n",
        "        self.captions_file = captions_file  # Added this line\n",
        "        self.max_len = max_len\n",
        "        self.cache_dir = cache_dir\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.vit_model = None\n",
        "\n",
        "        # Create cache directory\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "        # Cache file paths\n",
        "        self.vocab_cache = os.path.join(cache_dir, 'vocabulary.pkl')\n",
        "        self.train_cache = os.path.join(cache_dir, 'train_data.pkl')\n",
        "        self.test_cache = os.path.join(cache_dir, 'test_data.pkl')\n",
        "\n",
        "        # Initialize ViT model (do it once)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        print(\"Loading ViT model...\")\n",
        "        self.vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224').to(self.device)\n",
        "        self.vit_model.eval()\n",
        "\n",
        "    def extract_features(self, image_path):\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                outputs = self.vit_model(**inputs)\n",
        "                features = outputs.last_hidden_state[:, 0, :].cpu()\n",
        "                return features.squeeze(0)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {image_path}: {str(e)}\")\n",
        "            return torch.zeros(768)\n",
        "\n",
        "    def process_data(self):\n",
        "        if (os.path.exists(self.train_cache) and\n",
        "            os.path.exists(self.test_cache) and\n",
        "            os.path.exists(self.vocab_cache)):\n",
        "            print(\"Loading cached data...\")\n",
        "            return self.load_cached_data()\n",
        "\n",
        "        print(\"Processing data from scratch...\")\n",
        "        return self.create_and_cache_data()\n",
        "\n",
        "    def create_and_cache_data(self):\n",
        "        print(\"Reading captions file...\")\n",
        "        img_captions = {}\n",
        "        all_captions = []\n",
        "\n",
        "        # Read and process captions\n",
        "        with open(self.captions_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()[1:]  # Skip header\n",
        "            for line in tqdm(lines, desc=\"Reading captions\"):\n",
        "                parts = line.strip().split(',', 1)\n",
        "                if len(parts) == 2:\n",
        "                    img_name = parts[0].strip()\n",
        "                    caption = parts[1].strip().strip('\"\\'')\n",
        "\n",
        "                    if caption:\n",
        "                        if img_name not in img_captions:\n",
        "                            img_captions[img_name] = []\n",
        "                        img_captions[img_name].append(caption)\n",
        "                        all_captions.append(caption)\n",
        "\n",
        "        # Build vocabulary\n",
        "        word2idx, idx2word = self.build_vocabulary(all_captions)\n",
        "\n",
        "        # Process all images and captions\n",
        "        print(\"\\nProcessing images and creating batches...\")\n",
        "        features_list = []\n",
        "        captions_list = []\n",
        "\n",
        "        for img_name in tqdm(img_captions.keys(), desc=\"Processing images\"):\n",
        "            image_path = os.path.join(self.image_dir, img_name)\n",
        "            if os.path.exists(image_path):\n",
        "                features = self.extract_features(image_path)\n",
        "\n",
        "                for caption in img_captions[img_name]:\n",
        "                    features_list.append(features)\n",
        "\n",
        "                    # Process caption\n",
        "                    words = caption.lower().split()\n",
        "                    caption_indices = [word2idx.get(word, word2idx['<UNK>']) for word in words]\n",
        "                    caption_indices = [word2idx['<START>']] + caption_indices + [word2idx['<END>']]\n",
        "\n",
        "                    # Pad sequence\n",
        "                    if len(caption_indices) < self.max_len:\n",
        "                        caption_indices += [word2idx['<PAD>']] * (self.max_len - len(caption_indices))\n",
        "                    else:\n",
        "                        caption_indices = caption_indices[:self.max_len]\n",
        "\n",
        "                    captions_list.append(caption_indices)\n",
        "            else:\n",
        "                print(f\"Warning: Image not found: {image_path}\")\n",
        "\n",
        "        # Convert to tensors\n",
        "        print(\"\\nConverting to tensors...\")\n",
        "        features_tensor = torch.stack(features_list)\n",
        "        captions_tensor = torch.tensor(captions_list)\n",
        "\n",
        "        # Split into train and test\n",
        "        print(\"Splitting into train and test sets...\")\n",
        "        indices = torch.randperm(len(features_tensor))\n",
        "        train_size = int(0.8 * len(indices))\n",
        "\n",
        "        train_indices = indices[:train_size]\n",
        "        test_indices = indices[train_size:]\n",
        "\n",
        "        train_data = (features_tensor[train_indices], captions_tensor[train_indices])\n",
        "        test_data = (features_tensor[test_indices], captions_tensor[test_indices])\n",
        "\n",
        "        # Cache the processed data\n",
        "        print(\"Caching processed data...\")\n",
        "        with open(self.train_cache, 'wb') as f:\n",
        "            pickle.dump(train_data, f)\n",
        "        with open(self.test_cache, 'wb') as f:\n",
        "            pickle.dump(test_data, f)\n",
        "\n",
        "        print(f\"\\nProcessing completed!\")\n",
        "        print(f\"Train set size: {len(train_indices)}\")\n",
        "        print(f\"Test set size: {len(test_indices)}\")\n",
        "\n",
        "        return (word2idx, idx2word), train_data, test_data\n",
        "    def build_vocabulary(self, captions):\n",
        "        print(\"Building vocabulary...\")\n",
        "        word_freq = {}\n",
        "\n",
        "        for caption in captions:\n",
        "            words = caption.lower().split()\n",
        "            for word in words:\n",
        "                word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "        word2idx = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3}\n",
        "        for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True):\n",
        "            if len(word2idx) < 10000:\n",
        "                word2idx[word] = len(word2idx)\n",
        "\n",
        "        idx2word = {v: k for k, v in word2idx.items()}\n",
        "\n",
        "        # Cache vocabulary\n",
        "        with open(self.vocab_cache, 'wb') as f:\n",
        "            pickle.dump((word2idx, idx2word), f)\n",
        "\n",
        "        print(f\"Vocabulary size: {len(word2idx)}\")\n",
        "        return word2idx, idx2word\n",
        "    def load_cached_data(self):\n",
        "        print(\"Loading vocabulary...\")\n",
        "        with open(self.vocab_cache, 'rb') as f:\n",
        "            vocab = pickle.load(f)\n",
        "\n",
        "        print(\"Loading train data...\")\n",
        "        with open(self.train_cache, 'rb') as f:\n",
        "            train_data = pickle.load(f)\n",
        "\n",
        "        print(\"Loading test data...\")\n",
        "        with open(self.test_cache, 'rb') as f:\n",
        "            test_data = pickle.load(f)\n",
        "\n",
        "        print(f\"Train set size: {len(train_data[0])}\")\n",
        "        print(f\"Test set size: {len(test_data[0])}\")\n",
        "\n",
        "        return vocab, train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "ZHwI7ZugXvUJ",
        "outputId": "ee5e4ac4-8643-447f-dcf3-54263186804f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing ViT feature extractor...\n",
            "Using device: cuda\n",
            "Loading ViT model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data from scratch...\n",
            "Reading captions file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading captions: 100%|██████████| 158915/158915 [00:00<00:00, 751378.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building vocabulary...\n",
            "Vocabulary size: 10000\n",
            "\n",
            "Processing images and creating batches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   3%|▎         | 918/31783 [03:53<2:11:00,  3.93it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3f9f825e03e4>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_captions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# print(f\"Dataset size: {len(dataset)}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Split dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5178fcec85a5>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing data from scratch...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_and_cache_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_and_cache_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5178fcec85a5>\u001b[0m in \u001b[0;36mcreate_and_cache_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_captions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5178fcec85a5>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3440\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3442\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3444\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "    # Initialize ViT feature extractor\n",
        "print(\"Initializing ViT feature extractor...\")\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "    # Create dataset\n",
        "preprocessor = DataPreprocessor(\n",
        "        image_dir=\"/content/drive/MyDrive/Tech India/Preprocessed-Dataset/Rams-approach-preprocess/flickr30k/Images\",\n",
        "        captions_file=\"/content/drive/MyDrive/Tech India/Preprocessed-Dataset/Rams-approach-preprocess/flickr30k/captions.txt\",\n",
        "        feature_extractor=feature_extractor\n",
        "    )\n",
        "(word2idx, idx2word), (train_features, train_captions), (test_features, test_captions) = preprocessor.process_data()\n",
        "# print(f\"Dataset size: {len(dataset)}\")\n",
        "    # Split dataset\n",
        "train_dataset = TensorDataset(train_features, train_captions)\n",
        "test_dataset = TensorDataset(test_features, test_captions)\n",
        "    # Create data loaders\n",
        "train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=16,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=16,\n",
        "        pin_memory=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(word2idx, idx2word), (train_features, train_captions), (test_features, test_captions) = preprocessor.process_data()\n",
        "# print(f\"Dataset size: {len(dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "fZhC2ZBr4r9z",
        "outputId": "dfb2510b-7374-4960-e804-af40d3b28eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data from scratch...\n",
            "Reading captions file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading captions: 100%|██████████| 158915/158915 [00:00<00:00, 844690.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building vocabulary...\n",
            "Vocabulary size: 10000\n",
            "\n",
            "Processing images and creating batches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   4%|▍         | 1339/31783 [04:29<1:42:10,  4.97it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9e1fff0abdfa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_captions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(f\"Dataset size: {len(dataset)}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5178fcec85a5>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing data from scratch...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_and_cache_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_and_cache_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5178fcec85a5>\u001b[0m in \u001b[0;36mcreate_and_cache_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_captions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5178fcec85a5>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3440\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3442\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3444\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def load_dataset(cache_dir='cached_data', batch_size=32):\n",
        "    \"\"\"\n",
        "    Load preprocessed data from pickle files and create DataLoaders\n",
        "    Returns vocabulary and data loaders for train and test sets\n",
        "    \"\"\"\n",
        "    vocab_path = os.path.join(cache_dir, 'vocabulary.pkl')\n",
        "    train_path = os.path.join(cache_dir, 'train_data.pkl')\n",
        "    test_path = os.path.join(cache_dir, 'test_data.pkl')\n",
        "\n",
        "    # Check if pickle files exist\n",
        "    if not all(os.path.exists(p) for p in [vocab_path, train_path, test_path]):\n",
        "        raise FileNotFoundError(\"Required pickle files not found. Run preprocessing first.\")\n",
        "\n",
        "    # Load vocabulary\n",
        "    print(\"Loading vocabulary...\")\n",
        "    with open(vocab_path, 'rb') as f:\n",
        "        word2idx, idx2word = pickle.load(f)\n",
        "\n",
        "    # Load train features and captions\n",
        "    print(\"Loading train data...\")\n",
        "    with open(train_path, 'rb') as f:\n",
        "        train_features, train_captions = pickle.load(f)\n",
        "\n",
        "    # Load test features and captions\n",
        "    print(\"Loading test data...\")\n",
        "    with open(test_path, 'rb') as f:\n",
        "        test_features, test_captions = pickle.load(f)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TensorDataset(train_features, train_captions)\n",
        "    test_dataset = TensorDataset(test_features, test_captions)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=16,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=16,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"Vocabulary size: {len(word2idx)}\")\n",
        "    print(f\"Train set size: {len(train_features)}\")\n",
        "    print(f\"Test set size: {len(test_features)}\")\n",
        "\n",
        "    # Return both raw data and loaders\n",
        "    raw_data = {\n",
        "        'vocab': (word2idx, idx2word),\n",
        "        'train_data': (train_features, train_captions),\n",
        "        'test_data': (test_features, test_captions)\n",
        "    }\n",
        "\n",
        "    loaders = {\n",
        "        'train': train_loader,\n",
        "        'test': test_loader\n",
        "    }\n",
        "\n",
        "    return raw_data, loaders\n",
        "# Load both raw data and DataLoaders\n",
        "raw_data, loaders = load_dataset(cache_dir='/content/drive/MyDrive/Tech India/Preprocessed-Dataset/Rams-approach-preprocess/flickr30k/cached_data', batch_size=32)\n",
        "\n",
        "# Access vocabulary and raw data if needed\n",
        "word2idx, idx2word = raw_data['vocab']\n",
        "train_features, train_captions = raw_data['train_data']\n",
        "test_features, test_captions = raw_data['test_data']\n",
        "\n",
        "# Access DataLoaders\n",
        "train_loader = loaders['train']\n",
        "test_loader = loaders['test']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85eakc562dQ9",
        "outputId": "eebb1554-3110-46a0-89ee-f34e407ad782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading vocabulary...\n",
            "Loading train data...\n",
            "Loading test data...\n",
            "Vocabulary size: 10000\n",
            "Train set size: 127131\n",
            "Test set size: 31783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "wIqSaQMXU6Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.attention = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "        self.v.data.normal_(mean=0, std=0.1)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        hidden: (batch_size, 1, hidden_size)\n",
        "        encoder_outputs: (batch_size, seq_len, hidden_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, hidden_size = encoder_outputs.size()\n",
        "\n",
        "        # Ensure hidden has correct shape\n",
        "        if hidden.dim() == 2:\n",
        "            hidden = hidden.unsqueeze(1)\n",
        "\n",
        "        # Repeat hidden state for each encoder output\n",
        "        hidden = hidden.repeat(1, seq_len, 1)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        energy = torch.tanh(self.attention(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "\n",
        "        # Reshape v for batch processing\n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attention_weights = torch.bmm(v, energy.transpose(1, 2)).squeeze(1)\n",
        "        attention_weights = F.softmax(attention_weights, dim=1)\n",
        "\n",
        "        # Apply attention to encoder outputs\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
        "\n",
        "        return context, attention_weights\n"
      ],
      "metadata": {
        "id": "3V5mk2rQC3dH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "fhu_DD19Xh_j"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocab_size, embed_size=256, num_layers=2, dropout_p=0.3):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # Image feature processing\n",
        "        self.feature_encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p)\n",
        "        )\n",
        "\n",
        "        # Word embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.embed_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Additional embedding processing\n",
        "        self.embed_process = nn.Linear(embed_size, hidden_size)\n",
        "\n",
        "        # Attention\n",
        "        self.attention = AttentionLayer(hidden_size)\n",
        "\n",
        "        # Decoder LSTM\n",
        "        self.decoder_rnn = nn.LSTM(\n",
        "            input_size=hidden_size * 2,  # Concatenated context and processed embedding\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_p if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(hidden_size, vocab_size)\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, images, captions, teacher_forcing_ratio=0.5):\n",
        "        batch_size = images.size(0)\n",
        "        max_length = captions.size(1) - 1  # -1 because we don't predict for last token\n",
        "        device = images.device\n",
        "\n",
        "        # Encode images\n",
        "        image_features = self.feature_encoder(images)\n",
        "        image_features = image_features.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
        "\n",
        "        # Initialize outputs tensor\n",
        "        outputs = torch.zeros(batch_size, max_length, self.vocab_size).to(device)\n",
        "\n",
        "        # Initialize decoder input\n",
        "        decoder_input = captions[:, 0]  # Start tokens\n",
        "\n",
        "        # Initialize hidden states\n",
        "        h = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        c = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        hidden = (h, c)\n",
        "\n",
        "        for t in range(max_length):\n",
        "            # Embed input tokens\n",
        "            embedded = self.embedding(decoder_input)  # (batch_size, embed_size)\n",
        "            embedded = self.embed_dropout(embedded)\n",
        "            embedded = self.embed_process(embedded)  # (batch_size, hidden_size)\n",
        "\n",
        "            # Add sequence dimension\n",
        "            embedded = embedded.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
        "\n",
        "            # Calculate attention\n",
        "            context, _ = self.attention(embedded, image_features)\n",
        "\n",
        "            # Combine embedding and context\n",
        "            decoder_input_combined = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "            # RNN forward pass\n",
        "            output, hidden = self.decoder_rnn(decoder_input_combined, hidden)\n",
        "\n",
        "            # Process output\n",
        "            output = self.layer_norm(output.squeeze(1))\n",
        "            output = self.output_layer(output)\n",
        "\n",
        "            # Store output\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            # Teacher forcing\n",
        "            if random.random() < teacher_forcing_ratio and t < max_length - 1:\n",
        "                decoder_input = captions[:, t + 1]\n",
        "            else:\n",
        "                decoder_input = output.argmax(dim=1)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_model(model, test_loader, criterion, device, pad_idx):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_word_accuracy = 0\n",
        "    total_sentence_accuracy = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(test_loader, desc=\"Evaluating\")\n",
        "\n",
        "        for images, captions in progress_bar:\n",
        "            # Move to device\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            # Get input and target sequences\n",
        "            input_captions = captions[:, :-1]\n",
        "            target_captions = captions[:, 1:]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images, input_captions)\n",
        "            outputs = outputs[:, :-1, :]\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            outputs_flat = outputs.reshape(-1, outputs.size(-1))\n",
        "            targets_flat = target_captions.reshape(-1)\n",
        "\n",
        "            # Calculate metrics\n",
        "            loss = criterion(outputs_flat, targets_flat)\n",
        "            word_acc, sent_acc = calculate_accuracy(outputs_flat, targets_flat, pad_idx)\n",
        "\n",
        "            # Update metrics\n",
        "            total_loss += loss.item()\n",
        "            total_word_accuracy += word_acc\n",
        "            total_sentence_accuracy += sent_acc\n",
        "            num_batches += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'word_acc': f'{word_acc:.4f}',\n",
        "                'sent_acc': f'{sent_acc:.4f}'\n",
        "            })\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_word_acc = total_word_accuracy / num_batches\n",
        "    avg_sent_acc = total_sentence_accuracy / num_batches\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'word_accuracy': avg_word_acc,\n",
        "        'sentence_accuracy': avg_sent_acc\n",
        "    }"
      ],
      "metadata": {
        "id": "mg4rKUZl3Rvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(outputs, targets, pad_idx):\n",
        "    \"\"\"\n",
        "    Calculate word-level\n",
        "    outputs: (batch_size * seq_len, vocab_size)\n",
        "    targets: (batch_size * seq_len)\n",
        "    \"\"\"\n",
        "    # Get predictions\n",
        "    predictions = outputs.argmax(dim=1)  # (batch_size * seq_len)\n",
        "\n",
        "    # Create mask to ignore padding tokens\n",
        "    mask = (targets != pad_idx)\n",
        "\n",
        "    # Word-level accuracy\n",
        "    correct_words = ((predictions == targets) & mask).sum().item()\n",
        "    total_words = mask.sum().item()\n",
        "    word_accuracy = correct_words / total_words if total_words > 0 else 0\n",
        "\n",
        "    # Reshape for sentence-level accuracy\n",
        "    batch_size = len(targets) // targets.shape[0]\n",
        "    predictions = predictions.view(-1, batch_size)\n",
        "    targets = targets.view(-1, batch_size)\n",
        "    mask = mask.view(-1, batch_size)\n",
        "\n",
        "    return word_accuracy"
      ],
      "metadata": {
        "id": "I5h9CEoD3VJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.parallel import DataParallel\n",
        "from tqdm import tqdm\n",
        "import threading\n",
        "from queue import Queue\n",
        "import random\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ],
      "metadata": {
        "id": "Qn1Jr9Wl36Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGfC3vfCXkZY"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def train_model(model, train_loader, criterion, optimizer, device, epoch, total_epochs, teacher_forcing_ratio=0.5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_words = 0\n",
        "    correct_words = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{total_epochs}\")\n",
        "\n",
        "    for i, (images, captions) in enumerate(progress_bar):\n",
        "        try:\n",
        "            # Move to device\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images, captions)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(\n",
        "                outputs.reshape(-1, outputs.size(-1)),\n",
        "                captions[:, 1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = outputs.argmax(dim=2)\n",
        "            mask = captions[:, 1:] != 0  # Ignore padding\n",
        "            correct = (predictions == captions[:, 1:]) & mask\n",
        "            total_words += mask.sum().item()\n",
        "            correct_words += correct.sum().item()\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update metrics\n",
        "            total_loss += loss.item()\n",
        "            current_accuracy = correct_words / total_words if total_words > 0 else 0\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{current_accuracy:.4f}'\n",
        "            })\n",
        "\n",
        "            # Print batch statistics\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"\\nBatch {i+1}/{len(train_loader)}\")\n",
        "                print(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError in batch {i}:\")\n",
        "            print(f\"Exception: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_accuracy = correct_words / total_words if total_words > 0 else 0\n",
        "\n",
        "    return avg_loss, avg_accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import threading\n",
        "from queue import Queue\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, device, epoch, total_epochs, teacher_forcing_ratio=0.5, num_threads=2):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_words = 0\n",
        "    correct_words = 0\n",
        "\n",
        "    # Create queues for batch processing and results\n",
        "    batch_queue = Queue(maxsize=num_threads * 2)\n",
        "    result_queue = Queue()\n",
        "\n",
        "    # Lock for synchronizing updates\n",
        "    update_lock = threading.Lock()\n",
        "\n",
        "    def process_batch(batch_data):\n",
        "        try:\n",
        "            images, captions = batch_data\n",
        "            batch_results = {}\n",
        "\n",
        "            # Move to device\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images, captions)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(\n",
        "                outputs.reshape(-1, outputs.size(-1)),\n",
        "                captions[:, 1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = outputs.argmax(dim=2)\n",
        "            mask = captions[:, 1:] != 0  # Ignore padding\n",
        "            correct = (predictions == captions[:, 1:]) & mask\n",
        "\n",
        "            batch_results['loss'] = loss\n",
        "            batch_results['correct'] = correct.sum().item()\n",
        "            batch_results['total'] = mask.sum().item()\n",
        "\n",
        "            return batch_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError processing batch:\")\n",
        "            print(f\"Exception: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def update_metrics(results):\n",
        "        nonlocal total_loss, total_words, correct_words\n",
        "\n",
        "        with update_lock:\n",
        "            if results:\n",
        "                total_loss += results['loss'].item()\n",
        "                total_words += results['total']\n",
        "                correct_words += results['correct']\n",
        "\n",
        "                # Backward pass (needs to be done in main thread for thread safety)\n",
        "                optimizer.zero_grad()\n",
        "                results['loss'].backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{total_epochs}\")\n",
        "\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "        futures = []\n",
        "\n",
        "        for i, batch in enumerate(progress_bar):\n",
        "            future = executor.submit(process_batch, batch)\n",
        "            futures.append(future)\n",
        "\n",
        "            while futures:\n",
        "                done_futures = [f for f in futures if f.done()]\n",
        "                for future in done_futures:\n",
        "                    results = future.result()\n",
        "                    update_metrics(results)\n",
        "                    futures.remove(future)\n",
        "\n",
        "                    current_accuracy = correct_words / total_words if total_words > 0 else 0\n",
        "                    progress_bar.set_postfix({\n",
        "                        'loss': f'{total_loss/(i+1):.4f}',\n",
        "                        'acc': f'{current_accuracy:.4f}'\n",
        "                    })\n",
        "\n",
        "                    if (i + 1) % 500 == 0:\n",
        "                        print(f\"\\nBatch {i+1}/{len(train_loader)}\")\n",
        "                        print(f\"Loss: {total_loss/(i+1):.4f}\")\n",
        "                        print(f\"Accuracy: {current_accuracy:.4f}\")\n",
        "                        print(f\"Total words: {total_words}\")\n",
        "                        print(f\"Correct words: {correct_words}\")\n",
        "\n",
        "        for future in futures:\n",
        "            results = future.result()\n",
        "            update_metrics(results)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_accuracy = correct_words / total_words if total_words > 0 else 0\n",
        "\n",
        "    return avg_loss, avg_accuracy\n"
      ],
      "metadata": {
        "id": "ktSh0T_G5lot"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lL06zl4ofhVy",
        "outputId": "cbe8c4eb-6d86-4765-bf0c-66a8f26a9674"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Starting training...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5:  13%|█▎        | 500/3973 [06:43<2:13:21,  2.30s/it, loss=4.9630, acc=0.2323]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 500/3973\n",
            "Loss: 4.9630\n",
            "Accuracy: 0.2323\n",
            "Total words: 229577\n",
            "Correct words: 53325\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5:  25%|██▌       | 1000/3973 [13:09<29:53,  1.66it/s, loss=4.8020, acc=0.2440]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 1000/3973\n",
            "Loss: 4.8020\n",
            "Accuracy: 0.2440\n",
            "Total words: 458847\n",
            "Correct words: 111958\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5:  38%|███▊      | 1500/3973 [19:50<32:01,  1.29it/s, loss=4.7201, acc=0.2503]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 1500/3973\n",
            "Loss: 4.7201\n",
            "Accuracy: 0.2503\n",
            "Total words: 689023\n",
            "Correct words: 172447\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5:  50%|█████     | 2000/3973 [26:29<17:21,  1.90it/s, loss=4.6608, acc=0.2543]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 2000/3973\n",
            "Loss: 4.6608\n",
            "Accuracy: 0.2543\n",
            "Total words: 920647\n",
            "Correct words: 234164\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5:  63%|██████▎   | 2500/3973 [33:02<13:22,  1.83it/s, loss=4.6170, acc=0.2570]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 2500/3973\n",
            "Loss: 4.6170\n",
            "Accuracy: 0.2570\n",
            "Total words: 1150654\n",
            "Correct words: 295717\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5:  76%|███████▌  | 3000/3973 [39:32<45:03,  2.78s/it, loss=4.5782, acc=0.2597]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 3000/3973\n",
            "Loss: 4.5782\n",
            "Accuracy: 0.2597\n",
            "Total words: 1380961\n",
            "Correct words: 358703\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5:  88%|████████▊ | 3500/3973 [46:06<04:38,  1.70it/s, loss=4.5483, acc=0.2614]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 3500/3973\n",
            "Loss: 4.5483\n",
            "Accuracy: 0.2614\n",
            "Total words: 1610885\n",
            "Correct words: 421148\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|██████████| 3973/3973 [52:26<00:00,  1.26it/s, loss=4.5236, acc=0.2627]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "Average Loss: 4.5236\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5:  13%|█▎        | 500/3973 [06:21<31:20,  1.85it/s, loss=4.2617, acc=0.2759]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 500/3973\n",
            "Loss: 4.2617\n",
            "Accuracy: 0.2759\n",
            "Total words: 229742\n",
            "Correct words: 63394\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5:  25%|██▌       | 1000/3973 [12:55<25:02,  1.98it/s, loss=4.2575, acc=0.2757]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 1000/3973\n",
            "Loss: 4.2575\n",
            "Accuracy: 0.2757\n",
            "Total words: 460217\n",
            "Correct words: 126883\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5:  38%|███▊      | 1500/3973 [19:24<20:34,  2.00it/s, loss=4.2492, acc=0.2762]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 1500/3973\n",
            "Loss: 4.2492\n",
            "Accuracy: 0.2762\n",
            "Total words: 690722\n",
            "Correct words: 190748\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5:  50%|█████     | 2000/3973 [25:55<25:04,  1.31it/s, loss=4.2372, acc=0.2768]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 2000/3973\n",
            "Loss: 4.2372\n",
            "Accuracy: 0.2768\n",
            "Total words: 921838\n",
            "Correct words: 255157\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5:  63%|██████▎   | 2500/3973 [32:26<17:15,  1.42it/s, loss=4.2301, acc=0.2774]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 2500/3973\n",
            "Loss: 4.2301\n",
            "Accuracy: 0.2774\n",
            "Total words: 1150949\n",
            "Correct words: 319232\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5:  76%|███████▌  | 3000/3973 [38:54<22:25,  1.38s/it, loss=4.2188, acc=0.2782]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 3000/3973\n",
            "Loss: 4.2188\n",
            "Accuracy: 0.2782\n",
            "Total words: 1382234\n",
            "Correct words: 384555\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5:  88%|████████▊ | 3500/3973 [45:01<03:57,  1.99it/s, loss=4.2094, acc=0.2789]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 3500/3973\n",
            "Loss: 4.2094\n",
            "Accuracy: 0.2789\n",
            "Total words: 1612189\n",
            "Correct words: 449640\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 3973/3973 [51:05<00:00,  1.30it/s, loss=4.2024, acc=0.2794]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/10\n",
            "Average Loss: 4.2024\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5:  13%|█▎        | 500/3973 [06:38<2:10:33,  2.26s/it, loss=4.0922, acc=0.2849]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 500/3973\n",
            "Loss: 4.0922\n",
            "Accuracy: 0.2849\n",
            "Total words: 230820\n",
            "Correct words: 65763\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5:  25%|██▌       | 1000/3973 [13:02<32:40,  1.52it/s, loss=4.1037, acc=0.2826]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 1000/3973\n",
            "Loss: 4.1037\n",
            "Accuracy: 0.2826\n",
            "Total words: 460781\n",
            "Correct words: 130221\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5:  38%|███▊      | 1500/3973 [19:27<21:45,  1.89it/s, loss=4.0974, acc=0.2833]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 1500/3973\n",
            "Loss: 4.0974\n",
            "Accuracy: 0.2833\n",
            "Total words: 689977\n",
            "Correct words: 195479\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5:  50%|█████     | 2000/3973 [26:07<17:56,  1.83it/s, loss=4.0930, acc=0.2837]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 2000/3973\n",
            "Loss: 4.0930\n",
            "Accuracy: 0.2837\n",
            "Total words: 920251\n",
            "Correct words: 261056\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5:  63%|██████▎   | 2500/3973 [32:28<24:46,  1.01s/it, loss=4.0943, acc=0.2833]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 2500/3973\n",
            "Loss: 4.0943\n",
            "Accuracy: 0.2833\n",
            "Total words: 1150530\n",
            "Correct words: 325889\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5:  76%|███████▌  | 3000/3973 [38:48<08:39,  1.87it/s, loss=4.0930, acc=0.2832]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 3000/3973\n",
            "Loss: 4.0930\n",
            "Accuracy: 0.2832\n",
            "Total words: 1381473\n",
            "Correct words: 391238\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5:  88%|████████▊ | 3500/3973 [45:05<04:41,  1.68it/s, loss=4.0890, acc=0.2838]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 3500/3973\n",
            "Loss: 4.0890\n",
            "Accuracy: 0.2838\n",
            "Total words: 1612056\n",
            "Correct words: 457458\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|██████████| 3973/3973 [51:04<00:00,  1.30it/s, loss=4.0859, acc=0.2840]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/10\n",
            "Average Loss: 4.0859\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5:  13%|█▎        | 500/3973 [06:17<49:56,  1.16it/s, loss=4.0288, acc=0.2859]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 500/3973\n",
            "Loss: 4.0288\n",
            "Accuracy: 0.2859\n",
            "Total words: 230126\n",
            "Correct words: 65792\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5:  25%|██▌       | 1000/3973 [12:46<29:57,  1.65it/s, loss=4.0184, acc=0.2868]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 1000/3973\n",
            "Loss: 4.0184\n",
            "Accuracy: 0.2868\n",
            "Total words: 460656\n",
            "Correct words: 132131\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5:  38%|███▊      | 1500/3973 [19:08<1:52:27,  2.73s/it, loss=4.0155, acc=0.2876]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 1500/3973\n",
            "Loss: 4.0155\n",
            "Accuracy: 0.2876\n",
            "Total words: 690039\n",
            "Correct words: 198461\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5:  50%|█████     | 2000/3973 [25:25<18:11,  1.81it/s, loss=4.0111, acc=0.2879]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 2000/3973\n",
            "Loss: 4.0111\n",
            "Accuracy: 0.2879\n",
            "Total words: 920392\n",
            "Correct words: 264987\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5:  63%|██████▎   | 2500/3973 [31:51<11:49,  2.08it/s, loss=4.0116, acc=0.2878]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 2500/3973\n",
            "Loss: 4.0116\n",
            "Accuracy: 0.2878\n",
            "Total words: 1150577\n",
            "Correct words: 331152\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5:  76%|███████▌  | 3000/3973 [37:54<08:06,  2.00it/s, loss=4.0128, acc=0.2878]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 3000/3973\n",
            "Loss: 4.0128\n",
            "Accuracy: 0.2878\n",
            "Total words: 1380524\n",
            "Correct words: 397283\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5:  88%|████████▊ | 3500/3973 [44:17<03:54,  2.02it/s, loss=4.0131, acc=0.2877]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 3500/3973\n",
            "Loss: 4.0131\n",
            "Accuracy: 0.2877\n",
            "Total words: 1610687\n",
            "Correct words: 463453\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|██████████| 3973/3973 [50:27<00:00,  1.31it/s, loss=4.0125, acc=0.2876]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/10\n",
            "Average Loss: 4.0125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5:  13%|█▎        | 500/3973 [06:32<1:07:29,  1.17s/it, loss=3.9705, acc=0.2887]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 500/3973\n",
            "Loss: 3.9705\n",
            "Accuracy: 0.2887\n",
            "Total words: 230569\n",
            "Correct words: 66574\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5:  25%|██▌       | 1000/3973 [12:43<29:25,  1.68it/s, loss=3.9653, acc=0.2899]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 1000/3973\n",
            "Loss: 3.9653\n",
            "Accuracy: 0.2899\n",
            "Total words: 460448\n",
            "Correct words: 133501\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5:  38%|███▊      | 1500/3973 [19:09<23:04,  1.79it/s, loss=3.9640, acc=0.2902]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch 1500/3973\n",
            "Loss: 3.9640\n",
            "Accuracy: 0.2902\n",
            "Total words: 690405\n",
            "Correct words: 200329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5:  50%|█████     | 2000/3973 [25:15<17:03,  1.93it/s, loss=3.9663, acc=0.2901]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 2000/3973\n",
            "Loss: 3.9663\n",
            "Accuracy: 0.2901\n",
            "Total words: 920699\n",
            "Correct words: 267107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5:  63%|██████▎   | 2500/3973 [31:34<1:06:26,  2.71s/it, loss=3.9615, acc=0.2904]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 2500/3973\n",
            "Loss: 3.9615\n",
            "Accuracy: 0.2904\n",
            "Total words: 1150305\n",
            "Correct words: 334063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5:  76%|███████▌  | 3000/3973 [37:57<08:02,  2.02it/s, loss=3.9628, acc=0.2903]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 3000/3973\n",
            "Loss: 3.9628\n",
            "Accuracy: 0.2903\n",
            "Total words: 1380991\n",
            "Correct words: 400877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5:  88%|████████▊ | 3500/3973 [44:34<10:28,  1.33s/it, loss=3.9613, acc=0.2906]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 3500/3973\n",
            "Loss: 3.9613\n",
            "Accuracy: 0.2906\n",
            "Total words: 1610774\n",
            "Correct words: 468091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 3973/3973 [50:49<00:00,  1.30it/s, loss=3.9630, acc=0.2904]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/10\n",
            "Average Loss: 3.9630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/5:   4%|▍         | 156/3973 [01:59<48:39,  1.31it/s, loss=3.9552, acc=0.2885]  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-286246861ff2>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Main error: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-286246861ff2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         loss, accuracy = train_model(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-a8fcf7e15cba>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, device, epoch, total_epochs, teacher_forcing_ratio, num_threads)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdone_futures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mupdate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                     \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-a8fcf7e15cba>\u001b[0m in \u001b[0;36mupdate_metrics\u001b[0;34m(results)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# Backward pass (needs to be done in main thread for thread safety)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    LEARNING_RATE = 0.001\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    model = ImageCaptioningModel(\n",
        "        input_size=768,  # ViT base size\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        vocab_size=len(word2idx),\n",
        "        embed_size=EMBED_SIZE,\n",
        "        num_layers=2,\n",
        "        dropout_p=0.3\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        loss, accuracy = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        epoch=epoch+1,\n",
        "        total_epochs=5,\n",
        "        num_threads=16\n",
        "    )\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "        print(f\"Average Loss: {loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            'accuracy': accuracy,\n",
        "            'word2idx': word2idx,\n",
        "            'idx2word': idx2word\n",
        "        }, f'improved_model_epoch_{epoch+1}.pth')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        print(f\"Main error: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(image_path, model_path=\"best_model.pth\", max_length=50):\n",
        "    \"\"\"\n",
        "    Generate a caption for a single image using the saved model\n",
        "    \"\"\"\n",
        "    # Load model checkpoint\n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    word2idx = checkpoint['word2idx']\n",
        "    idx2word = checkpoint['idx2word']\n",
        "\n",
        "    # Initialize model and load weights\n",
        "    model = ImageCaptioningModel(\n",
        "        input_size=768,  # ViT base size\n",
        "        hidden_size=512,\n",
        "        vocab_size=len(word2idx)\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Load and process image\n",
        "    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "    vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "    # Extract features\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get ViT features\n",
        "        inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "        outputs = vit_model(**inputs)\n",
        "        image_features = outputs.last_hidden_state[:, 0, :]  # [1, 768]\n",
        "        print(f\"Image Embedding shape:\",{image_features.shape})\n",
        "    # Generate caption\n",
        "    with torch.no_grad():\n",
        "        # Initialize with start token\n",
        "        current_token = torch.tensor([[word2idx['<START>']]])\n",
        "        caption = []\n",
        "\n",
        "        # Generate words until max length or end token\n",
        "        for _ in range(max_length):\n",
        "            # Generate next word\n",
        "            output = model(image_features, current_token)\n",
        "            next_word_idx = output[0, -1].argmax().item()\n",
        "\n",
        "            # Convert to word\n",
        "            word = idx2word[next_word_idx]\n",
        "\n",
        "            # Stop if end token or pad\n",
        "            if word in ['<END>', '<PAD>']:\n",
        "                break\n",
        "\n",
        "            caption.append(word)\n",
        "\n",
        "            # Update current token\n",
        "            current_token = torch.cat([current_token, torch.tensor([[next_word_idx]])], dim=1)\n",
        "\n",
        "    return ' '.join(caption)\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Test with a sample image\n",
        "    image_path = \"/content/8192398089.jpg\"  # Replace with your image path\n",
        "    caption = generate_caption(image_path)\n",
        "    print(f\"\\nGenerated caption: {caption}\")\n",
        "\n",
        "    # # Test with multiple images\n",
        "    # test_images = [\n",
        "    #     \"flickr30k/images/image1.jpg\",\n",
        "    #     \"flickr30k/images/image2.jpg\",\n",
        "    #     \"flickr30k/images/image3.jpg\"\n",
        "    # ]\n",
        "\n",
        "    # print(\"\\nGenerating captions for multiple images:\")\n",
        "    # for img_path in test_images:\n",
        "    #     try:\n",
        "    #         caption = generate_caption(img_path)\n",
        "    #         print(f\"\\nImage: {img_path}\")\n",
        "    #         print(f\"Caption: {caption}\")\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error processing {img_path}: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGzKQTW99CBr",
        "outputId": "55f07684-78f9-423f-910c-5f6b6ebcf583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-38f54335be68>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path, map_location='cpu')\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Embedding shape: {torch.Size([1, 768])}\n",
            "\n",
            "Generated caption: women black in and outfits in dance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.attention = nn.Linear(hidden_size * 2, hidden_size)  # Matches checkpoint dimensions\n",
        "        self.v = nn.Parameter(torch.randn(hidden_size))  # Attention vector 'v'\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        seq_len = encoder_outputs.size(1)\n",
        "\n",
        "        # Repeat hidden state seq_len times\n",
        "        hidden = hidden.repeat(1, seq_len, 1)\n",
        "\n",
        "        # Concatenate hidden state with encoder outputs\n",
        "        inputs = torch.cat((hidden, encoder_outputs), dim=2)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attention_weights = torch.tanh(self.attention(inputs))  # [batch_size, seq_len, hidden_size]\n",
        "        attention_weights = torch.matmul(attention_weights, self.v)  # [batch_size, seq_len]\n",
        "        attention_weights = torch.softmax(attention_weights, dim=1).unsqueeze(1)\n",
        "\n",
        "        # Calculate context vector\n",
        "        context = torch.bmm(attention_weights, encoder_outputs)\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocab_size, embed_size=768, num_layers=2, dropout_p=0.3):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # Image feature processing\n",
        "        self.feature_encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size * 2),  # 768 -> 1024\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(hidden_size * 2, hidden_size),  # 1024 -> 512\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p)\n",
        "        )\n",
        "\n",
        "        # Word embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)  # 10000 x 768\n",
        "        self.embed_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Additional embedding processing\n",
        "        self.embed_process = nn.Linear(embed_size, hidden_size)  # 768 -> 512\n",
        "\n",
        "        # Attention\n",
        "        self.attention = AttentionLayer(hidden_size)  # 512\n",
        "\n",
        "        # Decoder LSTM (input: context + hidden = 1024)\n",
        "        self.decoder_rnn = nn.LSTM(\n",
        "            input_size=hidden_size * 2,  # 1024 (512 + 512)\n",
        "            hidden_size=hidden_size,     # 512\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_p if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),  # 512 -> 512\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(hidden_size, vocab_size)    # 512 -> 10000\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)  # 512\n",
        "\n",
        "    def generate_caption(self, image_features, word2idx, idx2word, max_length=50):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            batch_size = 1\n",
        "            device = image_features.device\n",
        "\n",
        "            # Encode image features\n",
        "            image_features = self.feature_encoder(image_features)\n",
        "            image_features = image_features.unsqueeze(1)  # [1, 1, 512]\n",
        "\n",
        "            # Initialize states\n",
        "            h = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "            c = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "            hidden = (h, c)\n",
        "\n",
        "            # Start with <START> token\n",
        "            decoder_input = torch.tensor([[word2idx['<START>']]]).to(device)\n",
        "\n",
        "            generated_words = []\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                # Embed input token\n",
        "                embedded = self.embedding(decoder_input)  # [1, 1, 768]\n",
        "                embedded = self.embed_dropout(embedded)\n",
        "                embedded = self.embed_process(embedded)  # [1, 1, 512]\n",
        "\n",
        "                # Calculate attention\n",
        "                context, _ = self.attention(embedded, image_features)  # [1, 1, 512]\n",
        "\n",
        "                # Combine embedding and context\n",
        "                decoder_input_combined = torch.cat((embedded, context), dim=2)  # [1, 1, 1024]\n",
        "\n",
        "                # RNN forward pass\n",
        "                output, hidden = self.decoder_rnn(decoder_input_combined, hidden)\n",
        "\n",
        "                # Process output\n",
        "                output = self.layer_norm(output.squeeze(1))  # [1, 512]\n",
        "                output = self.output_layer(output)  # [1, 10000]\n",
        "\n",
        "                # Get predicted word\n",
        "                predicted_idx = output.argmax(dim=1).item()\n",
        "                predicted_word = idx2word[predicted_idx]\n",
        "\n",
        "                if predicted_word in ['<END>', '<PAD>']:\n",
        "                    break\n",
        "\n",
        "                generated_words.append(predicted_word)\n",
        "                decoder_input = torch.tensor([[predicted_idx]]).to(device)\n",
        "\n",
        "            return generated_words\n",
        "\n",
        "# Rest of the evaluation code remains the same..."
      ],
      "metadata": {
        "id": "-3qJBJlajp7f"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "\n",
        "def load_image_paths_and_captions(image_dir, caption_file, max_images=5):\n",
        "    \"\"\"Load image paths and their corresponding captions from a text file\"\"\"\n",
        "    image_paths = []\n",
        "    reference_captions = []\n",
        "\n",
        "    with open(caption_file, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_images:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # Split by first comma only\n",
        "                parts = line.strip().split(',', 1)\n",
        "                if len(parts) == 2:\n",
        "                    image_name = parts[0].strip()\n",
        "                    caption = parts[1].strip()\n",
        "\n",
        "                    # Remove .jpg if present in image name\n",
        "                    if not image_name.endswith('.jpg'):\n",
        "                        image_name += '.jpg'\n",
        "\n",
        "                    image_path = os.path.join(image_dir, image_name)\n",
        "                    if os.path.exists(image_path):\n",
        "                        image_paths.append(image_path)\n",
        "                        reference_captions.append(caption)\n",
        "                        print(f\"Loaded: {image_name} with caption: {caption}\")\n",
        "                    else:\n",
        "                        print(f\"Warning: Image not found: {image_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing line {i+1}: {line.strip()}\")\n",
        "                print(f\"Error details: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    if not image_paths:\n",
        "        raise ValueError(\"No valid images found in the caption file\")\n",
        "\n",
        "    return image_paths, reference_captions\n",
        "\n",
        "def evaluate_model(model_path, image_dir, caption_file, device='cuda'):\n",
        "    \"\"\"Evaluate the model on specified images\"\"\"\n",
        "    print(\"Loading model...\")\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    word2idx = checkpoint['word2idx']\n",
        "    idx2word = checkpoint['idx2word']\n",
        "\n",
        "    # Initialize model\n",
        "    model = ImageCaptioningModel(\n",
        "        input_size=768,  # ViT feature size\n",
        "        hidden_size=512,\n",
        "        vocab_size=len(word2idx),\n",
        "        embed_size=768\n",
        "    )\n",
        "\n",
        "    # Load model weights\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize ViT models\n",
        "    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "    vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224').to(device)\n",
        "    vit_model.eval()\n",
        "\n",
        "    # Load image paths and captions\n",
        "    print(\"\\nLoading image paths and captions...\")\n",
        "    try:\n",
        "        image_paths, reference_captions = load_image_paths_and_captions(image_dir, caption_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading captions: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "    # Initialize BLEU score calculator\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    all_bleu_scores = []\n",
        "    all_references = []\n",
        "    all_hypotheses = []\n",
        "\n",
        "    print(\"\\nGenerating captions and calculating BLEU scores...\")\n",
        "    with torch.no_grad():\n",
        "        for i, (image_path, reference) in enumerate(zip(image_paths, reference_captions)):\n",
        "            try:\n",
        "                # Load and process image\n",
        "                image = Image.open(image_path).convert('RGB')\n",
        "                inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "                # Get ViT features\n",
        "                image_features = vit_model(**inputs).last_hidden_state[:, 0, :]\n",
        "\n",
        "                # Generate caption\n",
        "                generated_caption = model.generate_caption(image_features, word2idx, idx2word)\n",
        "\n",
        "                # Calculate BLEU scores\n",
        "                reference_tokens = reference.split()\n",
        "                bleu1 = sentence_bleu([reference_tokens], generated_caption,\n",
        "                                    weights=(1, 0, 0, 0),\n",
        "                                    smoothing_function=smoothing)\n",
        "                bleu4 = sentence_bleu([reference_tokens], generated_caption,\n",
        "                                    weights=(0.25, 0.25, 0.25, 0.25),\n",
        "                                    smoothing_function=smoothing)\n",
        "\n",
        "                all_bleu_scores.append({'bleu1': bleu1, 'bleu4': bleu4})\n",
        "                all_references.append([reference_tokens])\n",
        "                all_hypotheses.append(generated_caption)\n",
        "\n",
        "                # Print results\n",
        "                print(f\"\\nImage {i+1}: {os.path.basename(image_path)}\")\n",
        "                print(f\"Generated: {' '.join(generated_caption)}\")\n",
        "                print(f\"Reference: {reference}\")\n",
        "                print(f\"BLEU-1: {bleu1:.4f}\")\n",
        "                print(f\"BLEU-4: {bleu4:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image {image_path}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    # Calculate final scores\n",
        "    if all_bleu_scores:\n",
        "        avg_bleu1 = np.mean([s['bleu1'] for s in all_bleu_scores])\n",
        "        avg_bleu4 = np.mean([s['bleu4'] for s in all_bleu_scores])\n",
        "\n",
        "        corpus_bleu1 = corpus_bleu(all_references, all_hypotheses,\n",
        "                                 weights=(1, 0, 0, 0),\n",
        "                                 smoothing_function=smoothing)\n",
        "        corpus_bleu4 = corpus_bleu(all_references, all_hypotheses,\n",
        "                                 weights=(0.25, 0.25, 0.25, 0.25),\n",
        "                                 smoothing_function=smoothing)\n",
        "\n",
        "        print(\"\\nFinal Results:\")\n",
        "        print(f\"Average BLEU-1: {avg_bleu1:.4f}\")\n",
        "        print(f\"Average BLEU-4: {avg_bleu4:.4f}\")\n",
        "        print(f\"Corpus BLEU-1: {corpus_bleu1:.4f}\")\n",
        "        print(f\"Corpus BLEU-4: {corpus_bleu4:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'avg_bleu1': avg_bleu1,\n",
        "            'avg_bleu4': avg_bleu4,\n",
        "            'corpus_bleu1': corpus_bleu1,\n",
        "            'corpus_bleu4': corpus_bleu4\n",
        "        }\n",
        "    else:\n",
        "        print(\"No scores calculated.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = '/content/improved_model_epoch_4.pth'\n",
        "    IMAGE_DIR = '/content/drive/MyDrive/Tech India/Preprocessed-Dataset/Rams-approach-preprocess/flickr30k/Images'\n",
        "    CAPTION_FILE = '/content/drive/MyDrive/Tech India/Preprocessed-Dataset/Rams-approach-preprocess/flickr30k/captions.txt'\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Evaluate model\n",
        "    results = evaluate_model(MODEL_PATH, IMAGE_DIR, CAPTION_FILE, DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCqmtIpd_vke",
        "outputId": "330c6fa6-2462-4f50-c1bb-18a5dd276f8d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-a6a3a607fcf6>:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path, map_location=device)\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading image paths and captions...\n",
            "Warning: Image not found: /content/drive/MyDrive/Tech India/Preprocessed-Dataset/Rams-approach-preprocess/flickr30k/Images/image.jpg\n",
            "Loaded: 1000092795.jpg with caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "Loaded: 1000092795.jpg with caption: \" Two young , White males are outside near many bushes .\"\n",
            "Loaded: 1000092795.jpg with caption: Two men in green shirts are standing in a yard .\n",
            "Loaded: 1000092795.jpg with caption: A man in a blue shirt standing in a garden .\n",
            "\n",
            "Generating captions and calculating BLEU scores...\n",
            "\n",
            "Image 1: 1000092795.jpg\n",
            "Generated: a man in a a shirt is a a a .\n",
            "Reference: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "BLEU-1: 0.1054\n",
            "BLEU-4: 0.0130\n",
            "\n",
            "Image 2: 1000092795.jpg\n",
            "Generated: a man in a a shirt is a a a .\n",
            "Reference: \" Two young , White males are outside near many bushes .\"\n",
            "BLEU-1: 0.0000\n",
            "BLEU-4: 0.0000\n",
            "\n",
            "Image 3: 1000092795.jpg\n",
            "Generated: a man in a a shirt is a a a .\n",
            "Reference: Two men in green shirts are standing in a yard .\n",
            "BLEU-1: 0.2727\n",
            "BLEU-4: 0.0441\n",
            "\n",
            "Image 4: 1000092795.jpg\n",
            "Generated: a man in a a shirt is a a a .\n",
            "Reference: A man in a blue shirt standing in a garden .\n",
            "BLEU-1: 0.5455\n",
            "BLEU-4: 0.1109\n",
            "\n",
            "Final Results:\n",
            "Average BLEU-1: 0.2309\n",
            "Average BLEU-4: 0.0420\n",
            "Corpus BLEU-1: 0.2132\n",
            "Corpus BLEU-4: 0.0305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YCuMlTCp8GO",
        "outputId": "b6321c0b-5b1d-4307-8160-ff50b7bdd4bd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.53.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.53.0-py3-none-any.whl (387 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.1/387.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 openai-1.53.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import openai\n",
        "\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"/content/1000092795.jpg\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"What is in this image? and compare the description 'Two young , White males are outside near many bushes' to give score different from your output\",\n",
        "        },\n",
        "        {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\n",
        "            \"url\":  f\"data:image/jpeg;base64,{base64_image}\"\n",
        "          },\n",
        "        },\n",
        "      ],\n",
        "    }\n",
        "  ],\n",
        ")\n",
        "\n",
        "print(response.choices[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnY1LflTe-Q7",
        "outputId": "b9721f8b-ae74-4530-ed82-91dea0a4c134"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I can\\'t identify the people in the image, but I can describe the scene. The image shows two individuals standing near a door surrounded by lush greenery and bushes. \\n\\n**Comparison of descriptions:**\\n\\n- **Description provided:** \"Two young, White males are outside near many bushes.\"\\n- **My output:** \"Two individuals standing near a door surrounded by lush greenery and bushes.\"\\n\\n**Score:** 4/10 for similarity. The overall context is somewhat similar in that both mention two individuals and the outdoor setting with bushes; however, the specifics about their appearance and activities differ significantly, and my description does not indicate gender or ethnicity.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}