{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e643d158-5eb9-4ace-b239-5df53d394bef",
   "metadata": {},
   "source": [
    "Downloading the Dataset to folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901201cf-d7cd-45f0-ad2d-dcd06de6702f",
   "metadata": {},
   "source": [
    "!wget \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip\"\n",
    "!unzip -q flickr8k.zip -d ./flickr8k\n",
    "!rm flickr8k.zip\n",
    "!echo \"Downloaded Flickr8k dataset successfully.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca76a07-b0b0-4c6b-9232-9eedc79c9ea1",
   "metadata": {},
   "source": [
    "!wget \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr30k_part00\"\n",
    "!wget \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr30k_part01\"\n",
    "!wget \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr30k_part02\"\n",
    "!cat flickr30k_part00 flickr30k_part01 flickr30k_part02 > flickr30k.zip\n",
    "!rm flickr30k_part00 flickr30k_part01 flickr30k_part02\n",
    "!unzip -q flickr30k.zip -d ./flickr30k\n",
    "!rm flickr30k.zip\n",
    "!echo \"Downloaded Flickr30k dataset successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ec8ef98-9e10-46ea-b89f-f67f7193c9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kaggle in ./.local/lib/python3.8/site-packages (1.6.17)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in ./.local/lib/python3.8/site-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: tqdm in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from kaggle) (4.59.0)\n",
      "Requirement already satisfied: python-dateutil in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: bleach in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from kaggle) (3.3.0)\n",
      "Requirement already satisfied: urllib3 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from kaggle) (1.26.4)\n",
      "Requirement already satisfied: python-slugify in ./.local/lib/python3.8/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from kaggle) (2.25.1)\n",
      "Requirement already satisfied: six>=1.10 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: packaging in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from bleach->kaggle) (20.9)\n",
      "Requirement already satisfied: webencodings in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from packaging->bleach->kaggle) (2.4.7)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in ./.local/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests->kaggle) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests->kaggle) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a17e0005-c56d-4e6c-aa37-8689bc0a74ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /shared/centos7/anaconda3/2021.05/lib/libuuid.so.1: no version information available (required by wget)\n",
      "--2024-10-25 19:44:41--  http://images.cocodataset.org/zips/train2017.zip\n",
      "Connecting to 10.99.0.130:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 19336861798 (18G) [application/zip]\n",
      "Saving to: ‘coco_train2017.zip’\n",
      "\n",
      "100%[===================================>] 19,336,861,798 96.9MB/s   in 3m 13s \n",
      "\n",
      "2024-10-25 19:47:54 (95.4 MB/s) - ‘coco_train2017.zip’ saved [19336861798/19336861798]\n",
      "\n",
      "wget: /shared/centos7/anaconda3/2021.05/lib/libuuid.so.1: no version information available (required by wget)\n",
      "--2024-10-25 19:47:57--  http://images.cocodataset.org/zips/val2017.zip\n",
      "Connecting to 10.99.0.130:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 815585330 (778M) [application/zip]\n",
      "Saving to: ‘coco_val2017.zip’\n",
      "\n",
      "100%[======================================>] 815,585,330 94.8MB/s   in 8.2s   \n",
      "\n",
      "2024-10-25 19:48:05 (95.3 MB/s) - ‘coco_val2017.zip’ saved [815585330/815585330]\n",
      "\n",
      "wget: /shared/centos7/anaconda3/2021.05/lib/libuuid.so.1: no version information available (required by wget)\n",
      "--2024-10-25 19:48:06--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "Connecting to 10.99.0.130:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 252907541 (241M) [application/zip]\n",
      "Saving to: ‘coco_ann2017.zip’\n",
      "\n",
      "100%[======================================>] 252,907,541 96.2MB/s   in 2.5s   \n",
      "\n",
      "2024-10-25 19:48:09 (96.2 MB/s) - ‘coco_ann2017.zip’ saved [252907541/252907541]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://images.cocodataset.org/zips/train2017.zip -O coco_train2017.zip\n",
    "!wget http://images.cocodataset.org/zips/val2017.zip -O coco_val2017.zip\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O coco_ann2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c977ab-2008-42d5-bd87-8d592935306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Extract train images\n",
    "with zipfile.ZipFile('coco_train2017.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('coco/train2017')\n",
    "\n",
    "# Extract validation images\n",
    "with zipfile.ZipFile('coco_val2017.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('coco/val2017')\n",
    "\n",
    "# Extract annotations\n",
    "with zipfile.ZipFile('coco_ann2017.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('coco/annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd920f7b-a536-4599-9fde-a86b482373d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.remove('coco_train2017.zip')\n",
    "os.remove('coco_val2017.zip')\n",
    "os.remove('coco_ann2017.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10141fb-aa95-49da-b52c-453b4e02990c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa8cad-a9a9-4087-b3c8-ee903ab4f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision pandas numpy os transformers nltk json logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b771cd-9529-4bb2-b541-7cb90a3326e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from transformers import ViTFeatureExtractor, ViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd617000-2e81-42df-9900-583de11502a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import json\n",
    "import nltk\n",
    "from typing import List, Dict, Tuple\n",
    "import logging\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1804d44-2e6c-4c75-b3a0-a945204fd095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c288b15-db13-44ed-97c2-3fb0d86a9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, config_path: str = 'config.yaml'):\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            \n",
    "        # Model parameters\n",
    "        self.batch_size = config['model']['batch_size']\n",
    "        self.embed_size = config['model']['embed_size']\n",
    "        self.hidden_size = config['model']['hidden_size']\n",
    "        self.num_layers = config['model']['num_layers']\n",
    "        self.learning_rate = config['model']['learning_rate']\n",
    "        self.num_epochs = config['model']['num_epochs']\n",
    "        self.max_length = config['model']['max_length']\n",
    "        \n",
    "        # Dataset parameters\n",
    "        self.datasets = config['datasets']\n",
    "        self.min_word_freq = config['data']['min_word_freq']\n",
    "        \n",
    "        # Training parameters\n",
    "        self.checkpoint_dir = config['training']['checkpoint_dir']\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64037d-6bd0-452c-9fc9-7208d6fa1ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d52925-5149-4c3b-878c-ae022806ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size: int):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.linear = nn.Linear(768, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.vit(images).last_hidden_state[:, 0, :]\n",
    "        features = self.dropout(features)\n",
    "        features = self.linear(features)\n",
    "        return features\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, num_layers: int):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor) -> torch.Tensor:\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea5292-7a05-4963-a509-586aba4346eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transforms() -> transforms.Compose:\n",
    "    \"\"\"Prepare image transformations.\"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "class CombinedCaptionDataset(Dataset):\n",
    "    def __init__(self, dataset_configs: List[Dict], transform=None, min_word_freq: int = 5):\n",
    "        self.transform = transform\n",
    "        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.dataset_configs = dataset_configs\n",
    "        self.min_word_freq = min_word_freq\n",
    "        \n",
    "        # Combine all captions data\n",
    "        self.data = self._combine_datasets()\n",
    "        self.build_vocabulary()\n",
    "        \n",
    "    def _combine_datasets(self) -> pd.DataFrame:\n",
    "        \"\"\"Combine multiple datasets into a single DataFrame.\"\"\"\n",
    "        all_data = []\n",
    "        for dataset_config in self.dataset_configs:\n",
    "            df = pd.read_csv(dataset_config['captions_file'], delimiter=',')\n",
    "            df['image'] = df['image'].apply(\n",
    "                lambda x: os.path.join(dataset_config['image_dir'], x)\n",
    "            )\n",
    "            df['dataset_source'] = dataset_config['name']\n",
    "            all_data.append(df)\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "        \"\"\"Build vocabulary from all datasets.\"\"\"\n",
    "        word_freq = Counter()\n",
    "        for caption in self.data['caption']:\n",
    "            words = str(caption).lower().split()\n",
    "            word_freq.update(words)\n",
    "            \n",
    "        self.word2idx = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3}\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= self.min_word_freq:\n",
    "                self.word2idx[word] = len(self.word2idx)\n",
    "                \n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        logger.info(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        \n",
    "    def tokenize_caption(self, caption: str) -> List[int]:\n",
    "        \"\"\"Convert caption to token indices.\"\"\"\n",
    "        words = str(caption).lower().split()\n",
    "        tokens = []\n",
    "        tokens.append(self.word2idx['<START>'])\n",
    "        tokens.extend([self.word2idx.get(word, self.word2idx['<UNK>']) for word in words])\n",
    "        tokens.append(self.word2idx['<END>'])\n",
    "        return tokens\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        img_path = self.data.iloc[idx]['image']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        image_tensor = inputs['pixel_values'].squeeze(0)\n",
    "        \n",
    "        caption = self.data.iloc[idx]['caption']\n",
    "        tokens = self.tokenize_caption(caption)\n",
    "        \n",
    "        return image_tensor, torch.tensor(tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1480c29-d869-4a6a-8ec4-c1606f8eb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageCaptioningModel:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        self.transform = prepare_transforms()\n",
    "        \n",
    "        # Initialize dataset\n",
    "        self.dataset = CombinedCaptionDataset(\n",
    "            config.datasets,\n",
    "            transform=self.transform,\n",
    "            min_word_freq=config.min_word_freq\n",
    "        )\n",
    "        \n",
    "        # Initialize models\n",
    "        self.encoder = EncoderCNN(config.embed_size).to(self.device)\n",
    "        self.decoder = DecoderRNN(\n",
    "            self.dataset.vocab_size,\n",
    "            config.embed_size,\n",
    "            config.hidden_size,\n",
    "            config.num_layers\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.dataset.word2idx['<PAD>'])\n",
    "        self.encoder_optimizer = torch.optim.Adam(self.encoder.parameters(), lr=config.learning_rate)\n",
    "        self.decoder_optimizer = torch.optim.Adam(self.decoder.parameters(), lr=config.learning_rate)\n",
    "        \n",
    "        # Initialize data loader\n",
    "        self.train_loader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4\n",
    "        )\n",
    "        \n",
    "    def train_epoch(self) -> float:\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (images, captions) in enumerate(self.train_loader):\n",
    "            images = images.to(self.device)\n",
    "            captions = captions.to(self.device)\n",
    "            \n",
    "            self.encoder_optimizer.zero_grad()\n",
    "            self.decoder_optimizer.zero_grad()\n",
    "            \n",
    "            features = self.encoder(images)\n",
    "            outputs = self.decoder(features, captions[:, :-1])\n",
    "            \n",
    "            loss = self.criterion(\n",
    "                outputs.reshape(-1, outputs.shape[2]),\n",
    "                captions[:, 1:].reshape(-1)\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            self.encoder_optimizer.step()\n",
    "            self.decoder_optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                logger.info(f'Batch [{i}/{len(self.train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        return total_loss / len(self.train_loader)\n",
    "    \n",
    "    def save_checkpoint(self, epoch: int):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint_path = os.path.join(self.config.checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'encoder_state_dict': self.encoder.state_dict(),\n",
    "            'decoder_state_dict': self.decoder.state_dict(),\n",
    "            'encoder_optimizer': self.encoder_optimizer.state_dict(),\n",
    "            'decoder_optimizer': self.decoder_optimizer.state_dict(),\n",
    "            'vocab': self.dataset.word2idx,\n",
    "        }, checkpoint_path)\n",
    "        logger.info(f'Checkpoint saved: {checkpoint_path}')\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the model for specified number of epochs.\"\"\"\n",
    "        logger.info(f\"Starting training on device: {self.device}\")\n",
    "        logger.info(f\"Total number of samples: {len(self.dataset)}\")\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            logger.info(f'Epoch [{epoch+1}/{self.config.num_epochs}]')\n",
    "            train_loss = self.train_epoch()\n",
    "            logger.info(f'Training Loss: {train_loss:.4f}')\n",
    "            self.save_checkpoint(epoch)\n",
    "    \n",
    "    def generate_caption(self, image_path: str) -> str:\n",
    "        \"\"\"Generate caption for a new image.\"\"\"\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        image = image.unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(image)\n",
    "            \n",
    "            # Generate caption\n",
    "            caption = []\n",
    "            word_id = self.dataset.word2idx['<START>']\n",
    "            \n",
    "            for _ in range(self.config.max_length):\n",
    "                word_tensor = torch.tensor([word_id]).to(self.device)\n",
    "                output = self.decoder(features, word_tensor.unsqueeze(0))\n",
    "                word_id = output.argmax(2)[-1].item()\n",
    "                \n",
    "                if word_id == self.dataset.word2idx['<END>']:\n",
    "                    break\n",
    "                    \n",
    "                caption.append(self.dataset.idx2word[word_id])\n",
    "        \n",
    "        return ' '.join(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f24367-3d61-4294-86dc-e2d08e9fbafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Load configuration\n",
    "    config = Config('config.yaml')\n",
    "    \n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize and train model\n",
    "    model = ImageCaptioningModel(config)\n",
    "    model.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
